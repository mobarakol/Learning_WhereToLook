{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learning_Wheretolook_Demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3e1716048e10435a9e2642d1f1d347dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0458929256e2475ab608127df18e37f8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f3e4d8c468934a8a8f3bdee4bf049340",
              "IPY_MODEL_51877bb1fa8741a3ac6b645297d465a5"
            ]
          }
        },
        "0458929256e2475ab608127df18e37f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f3e4d8c468934a8a8f3bdee4bf049340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4bf485a52d964d15b0ef17ba78896132",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_65586baa43c045d28f96639efecf31df"
          }
        },
        "51877bb1fa8741a3ac6b645297d465a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ef7e6a26fd0e42a980153d7a85f3c4ad",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:10&lt;00:00, 4.41MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_94ccaba4a96d481d88f0b477a9d39e4b"
          }
        },
        "4bf485a52d964d15b0ef17ba78896132": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "65586baa43c045d28f96639efecf31df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ef7e6a26fd0e42a980153d7a85f3c4ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "94ccaba4a96d481d88f0b477a9d39e4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/Learning_WhereToLook/blob/master/Learning_Wheretolook_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUJMdlmnIDSP"
      },
      "source": [
        "# Learning Where to Look While Tracking Instruments in Robot-assisted Surgery\n",
        "\n",
        "Directing of the task-specific attention while tracking instrument in surgery holds great potential in robot-assisted intervention. For this purpose, we propose an end-to-end trainable multitask learning (MTL) model for real-time surgical instrument segmentation and attention prediction. Our model is designed with a weight-shared encoder and two task-oriented decoders and optimized for the joint tasks. We introduce batch-Wasserstein (bW) loss and construct a soft attention module to refine the distinctive visual region for efficient saliency learning. For multitask optimization, it is always challenging to obtain convergence of both tasks in the same epoch. We deal with this problem by adopting `poly' loss weight and two phases of training. We further propose a novel way to generate task-aware saliency map and scanpath of the instruments on MICCAI robotic instrument segmentation dataset. Compared to the state of the art segmentation and saliency models, our model outperforms most of the evaluation metrics.\n",
        "\n",
        "Paper: [Learning Where to Look While Tracking Instruments in Robot-assisted Surgery](https://link.springer.com/chapter/10.1007/978-3-030-32254-0_46) <br>\n",
        "Code: https://github.com/mobarakol/Learning_WhereToLook<br>\n",
        "\n",
        "Instrument Classes:\"Bipolar Forceps\": 1, \"Prograsp Forceps\": 2, \"Large Needle Driver\": 3, \"Vessel Sealer\": 4, \"Grasping Retractor\": 5, \"Monopolar Curve, Scissors\": 6, \"Other\": 7<br>\n",
        "\n",
        "\n",
        "## Citation\n",
        "If you use this code for your research, please cite our paper.\n",
        "\n",
        "```\n",
        "@inproceedings{islam2019learning,\n",
        "  title={Learning where to look while tracking instruments in robot-assisted surgery},\n",
        "  author={Islam, Mobarakol and Li, Yueyuan and Ren, Hongliang},\n",
        "  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},\n",
        "  pages={412--420},\n",
        "  year={2019},\n",
        "  organization={Springer}\n",
        "}\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diuXaiY0Jygj"
      },
      "source": [
        "Download Code, Data and Trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpBPJJFxKjdU"
      },
      "source": [
        "Download Code from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsSeawSLKAQl",
        "outputId": "68806f59-b16c-47bf-a66b-664a9129b489"
      },
      "source": [
        "!rm -rf Learning_WhereToLook\n",
        "!git clone https://github.com/mobarakol/Learning_WhereToLook.git\n",
        "%cd Learning_WhereToLook"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Learning_WhereToLook'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 20 (delta 2), reused 16 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (20/20), done.\n",
            "/content/Learning_WhereToLook\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_YO46L3K-Jh"
      },
      "source": [
        "Download Validation Data and Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7Igmt98H-cc"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_rKIlv3K04W"
      },
      "source": [
        "ids = ['1YcXNOMZOiqgLUXZf_oCCbyuicVJRNR1J', '1EeEhbgNV9K1Sm9AUst2VeQYiT5TZmVjT']\n",
        "zip_files = ['instruments17_saliency.zip','epoch_75.pth.tar']\n",
        "for id, zip_file in zip(ids, zip_files):\n",
        "    downloaded = drive.CreateFile({'id':id}) \n",
        "    downloaded.GetContentFile(zip_file)\n",
        "    if zip_file[-3:] == 'zip':\n",
        "        !unzip -q $zip_file"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjk7whIFL5fB"
      },
      "source": [
        "## Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154,
          "referenced_widgets": [
            "3e1716048e10435a9e2642d1f1d347dd",
            "0458929256e2475ab608127df18e37f8",
            "f3e4d8c468934a8a8f3bdee4bf049340",
            "51877bb1fa8741a3ac6b645297d465a5",
            "4bf485a52d964d15b0ef17ba78896132",
            "65586baa43c045d28f96639efecf31df",
            "ef7e6a26fd0e42a980153d7a85f3c4ad",
            "94ccaba4a96d481d88f0b477a9d39e4b"
          ]
        },
        "id": "sXX5AMfKLc-2",
        "outputId": "cba447b8-68c1-44b1-e269-8dfa71ccaa8d"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from model import SalSegNet\n",
        "from dataset import SurgicalDataset\n",
        "from utils import seed_everything, calculate_dice, calculate_confusion_matrix_from_arrays\n",
        "from metrics_functions import AUC_Borji, NSS, SIM\n",
        "from scipy.stats import wasserstein_distance\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def validate(valid_loader, model, args):\n",
        "    confusion_matrix = np.zeros(\n",
        "            (args.num_classes, args.num_classes), dtype=np.uint32)\n",
        "    model.eval()\n",
        "    SIM_SCORE, EMD_SCORE = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, labels_seg, labels_sal,_) in enumerate(valid_loader):\n",
        "            inputs, labels_seg = inputs.to(device), np.array(labels_seg)\n",
        "            pred_seg, output_sal = model(inputs)\n",
        "            pred_seg = pred_seg.data.max(1)[1].squeeze_(1).cpu().numpy()\n",
        "            confusion_matrix += calculate_confusion_matrix_from_arrays(\n",
        "                pred_seg, labels_seg, args.num_classes)\n",
        "            \n",
        "            pred_ = output_sal.view(output_sal.size()[0], -1)\n",
        "            gt_maps_ = labels_sal.view(labels_sal.size()[0], -1)\n",
        "            labels_sal = np.array(labels_sal.cpu())\n",
        "            output_sal = np.array(output_sal.cpu())\n",
        "            for eval_idx in range(0, inputs.size()[0]):\n",
        "                if(np.max(labels_sal[eval_idx])==0):\n",
        "                    continue\n",
        "                SIM_SCORE.append((SIM(labels_sal[eval_idx], np.squeeze(output_sal[eval_idx]))))\n",
        "                EMD_SCORE.append((wasserstein_distance(pred_[eval_idx].cpu(), gt_maps_[eval_idx].cpu())))   \n",
        "\n",
        "    confusion_matrix = confusion_matrix[1:, 1:]  # exclude background\n",
        "    dices = {'dice_{}'.format(cls + 1): dice\n",
        "                for cls, dice in enumerate(calculate_dice(confusion_matrix))}\n",
        "    dices_per_class = np.array(list(dices.values()))          \n",
        "\n",
        "    return dices_per_class, SIM_SCORE, EMD_SCORE\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Instrument Segmentation and Saliency')\n",
        "    parser.add_argument('--num_classes', default=8, type=int, help=\"num of classes\")\n",
        "    parser.add_argument('--data_root', default='instruments17_saliency', help=\"data root dir\")\n",
        "    parser.add_argument('--batch_size', default=2, type=int, help=\"num of classes\")\n",
        "    args = parser.parse_args(args=[])\n",
        "    dataset_test = SurgicalDataset(data_root=args.data_root, seq_set=[4,7], is_train=False)\n",
        "    test_loader = DataLoader(dataset=dataset_test, batch_size=args.batch_size, shuffle=False, num_workers=2,\n",
        "                              drop_last=True)\n",
        "    \n",
        "    print('Sample size of test dataset:', dataset_test.__len__())\n",
        "    model = SalSegNet(num_classes=args.num_classes).to(device)\n",
        "    model = torch.nn.parallel.DataParallel(model)\n",
        "    model.load_state_dict(torch.load('epoch_75.pth.tar'))\n",
        "    dices_per_class, SIM_SCORE, EMD_SCORE = validate(test_loader, model, args)\n",
        "    print('Mean Avg Dice:%.4f [Bipolar Forceps:%.4f, Prograsp Forceps:%.4f, Large Needle Driver:%.4f, Vessel Sealer:%.4f]'\n",
        "        %(dices_per_class[:4].mean(),dices_per_class[0], dices_per_class[1],dices_per_class[2],dices_per_class[3]))\n",
        "    print('Saliency Metrics: SIM:%.4f, EMD:%.4f'%(np.mean(SIM_SCORE), np.mean(EMD_SCORE)))\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    class_names = [\"Bipolar Forceps\", \"Prograsp Forceps\", \"Large Needle Driver\", \"Vessel Sealer\", \"Grasping Retractor\", \"Monopolar Curve, Scissors\", \"Other\"]\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    seed_everything()\n",
        "    main()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample size of test dataset: 441\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e1716048e10435a9e2642d1f1d347dd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Mean Avg Dice:0.6579 [Bipolar Forceps:0.4533, Prograsp Forceps:0.6918, Large Needle Driver:0.7998, Vessel Sealer:0.6868]\n",
            "Saliency Metrics: SIM:0.5258, EMD:0.0085\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msRgazO4_t0K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}